{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./data/Original.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "def ordinal_quantification(df, columns):\n",
    "    thresholds_dict = {}\n",
    "\n",
    "    for col in columns:\n",
    "        ordinal_data = df[col]\n",
    "        \n",
    "        # Count the occurrences of each category\n",
    "        unique_vals, counts = np.unique(ordinal_data, return_counts=True)\n",
    "        \n",
    "        # Calculate probabilities pi = ni / N\n",
    "        N = len(ordinal_data)\n",
    "        probabilities = counts / N\n",
    "        \n",
    "        # Initialize arrays to store thresholds and quantification values\n",
    "        thresholds = np.zeros(len(unique_vals) + 1)  # One more threshold than categories\n",
    "        quant_values = np.zeros(len(unique_vals))    # Quantified values for each category\n",
    "        \n",
    "        # Calculate thresholds ti for each category\n",
    "        cumulative_prob = 0\n",
    "        for i in range(1, len(probabilities)):\n",
    "            cumulative_prob += probabilities[i-1]\n",
    "            thresholds[i] = norm.ppf(cumulative_prob)\n",
    "        \n",
    "        thresholds[0] = -np.inf  # The first threshold is negative infinity\n",
    "        thresholds[-1] = np.inf  # The last threshold is positive infinity\n",
    "        \n",
    "        # Calculate quantification values qi for each category using the average probability\n",
    "        cumulative_prob = 0\n",
    "        for i in range(len(probabilities)):\n",
    "            cumulative_prob += probabilities[i] / 2  # Average probability for category\n",
    "            quant_values[i] = norm.ppf(cumulative_prob)\n",
    "            cumulative_prob += probabilities[i] / 2  # Complete probability after category\n",
    "        \n",
    "        # Map quantification values back to the DataFrame in place\n",
    "        quant_map = dict(zip(unique_vals, quant_values))\n",
    "        df[col] = ordinal_data.map(quant_map)\n",
    "        \n",
    "        # Store the thresholds for this column\n",
    "        thresholds_dict[col] = thresholds[1:-1]  # Exclude -inf and +inf\n",
    "\n",
    "    return df, thresholds_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Age': array([-0.40942456,  0.24374663,  0.78969104,  1.56419456,  2.34360974]), 'Education': array([-2.17395889, -1.49563802, -1.38328254, -1.09691387, -0.2410078 ,\n",
      "        0.12063672,  0.85113393,  1.67248   ]), 'Nscore': array([-3.273829  , -3.07260159, -2.5917443 , -2.46260694, -2.38722437,\n",
      "       -2.30404426, -2.14653601, -1.97027933, -1.78494304, -1.61108222,\n",
      "       -1.49563802, -1.38675251, -1.27403075, -1.1215144 , -0.9892386 ,\n",
      "       -0.85687868, -0.72935834, -0.62885655, -0.53281095, -0.40364723,\n",
      "       -0.29338123, -0.20012949, -0.09789469, -0.00598403,  0.09121601,\n",
      "        0.18117092,  0.26710292,  0.35930973,  0.47542685,  0.56840312,\n",
      "        0.69339187,  0.77883949,  0.87428537,  0.94883297,  1.09934402,\n",
      "        1.16759338,  1.30768129,  1.44470376,  1.5419863 ,  1.67248   ,\n",
      "        1.77201195,  1.91751597,  2.06147974,  2.20312121,  2.38722437,\n",
      "        2.55528529,  2.6770635 ,  3.07260159]), 'Escore': array([-3.07260159, -2.94948659, -2.5917443 , -2.49117622, -2.41085626,\n",
      "       -2.25072149, -2.17395889, -2.06147974, -2.01886429, -1.83990291,\n",
      "       -1.69441908, -1.57789881, -1.44470376, -1.31396074, -1.15714076,\n",
      "       -1.0313275 , -0.87039445, -0.74506335, -0.64678379, -0.50693334,\n",
      "       -0.37495719, -0.22734028, -0.08320693,  0.08988077,  0.24648729,\n",
      "        0.39932307,  0.55594371,  0.72416325,  0.89196108,  1.03814137,\n",
      "        1.1970175 ,  1.38675251,  1.52901814,  1.64614093,  1.85455025,\n",
      "        2.03970908,  2.23428706,  2.43591654,  2.78790317,  2.94948659,\n",
      "        3.07260159]), 'Oscore': array([-3.07260159, -2.72830624, -2.55528529, -2.28554028, -2.14653601,\n",
      "       -2.03970908, -1.91751597, -1.75315388, -1.61597002, -1.49971979,\n",
      "       -1.35610327, -1.20247972, -1.04271086, -0.91395079, -0.78425372,\n",
      "       -0.65335437, -0.51602701, -0.38926216, -0.24785832, -0.1085898 ,\n",
      "        0.06987006,  0.21371511,  0.37495719,  0.51906771,  0.65006557,\n",
      "        0.80063639,  0.97203229,  1.1623512 ,  1.32667751,  1.56419456,\n",
      "        1.76565599,  2.03970908,  2.30404426,  2.6770635 ]), 'Ascore': array([-3.273829  , -3.07260159, -2.94948659, -2.85942992, -2.72830624,\n",
      "       -2.6770635 , -2.43591654, -2.28554028, -2.16004442, -2.00876243,\n",
      "       -1.85455025, -1.70003327, -1.55077795, -1.41513411, -1.27703041,\n",
      "       -1.15196156, -1.004538  , -0.83595001, -0.69001353, -0.52669057,\n",
      "       -0.3820999 , -0.22324841, -0.08721077,  0.05255034,  0.21099491,\n",
      "        0.36641108,  0.51299108,  0.67157113,  0.85687868,  1.03359346,\n",
      "        1.20247972,  1.37982914,  1.52901814,  1.70570155,  1.96109987,\n",
      "        2.1334083 ,  2.36485492,  2.5917443 ,  3.07260159,  3.273829  ]), 'Cscore': array([-3.273829  , -3.07260159, -2.78790317, -2.6770635 , -2.49117622,\n",
      "       -2.36485492, -2.25072149, -2.12063827, -1.97962788, -1.86960665,\n",
      "       -1.70570155, -1.58253341, -1.4599745 , -1.31711999, -1.20247972,\n",
      "       -1.07770123, -0.95510906, -0.84541713, -0.72071069, -0.58725989,\n",
      "       -0.4694797 , -0.34374976, -0.20963539, -0.07653679,  0.06320643,\n",
      "        0.18387514,  0.3367047 ,  0.4978814 ,  0.676578  ,  0.84541713,\n",
      "        1.04271086,  1.23603481,  1.38328254,  1.55077795,  1.72304362,\n",
      "        1.91751597,  2.21843482,  2.49117622,  2.85942992,  3.273829  ]), 'SS': array([-1.77844032, -1.37982914, -1.02007623, -0.69508401, -0.37068075,\n",
      "       -0.06587153,  0.22734028,  0.58884066,  0.9699017 ,  1.60142048]), 'Impulsivity': array([-2.30404426, -1.00674287, -0.46799551,  0.02061299,  0.37068075,\n",
      "        0.70357479,  1.0920729 ,  1.56419456,  2.6770635 ])}\n"
     ]
    }
   ],
   "source": [
    "# Quantize columns Age\n",
    "columns = ['Age','Education','Nscore','Escore','Oscore','Ascore','Cscore','SS','Impulsivity']\n",
    "quantified_df, thresholds_dict = ordinal_quantification(df, columns)\n",
    "print(thresholds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Education</th>\n",
       "      <th>Country</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Nscore</th>\n",
       "      <th>Escore</th>\n",
       "      <th>Oscore</th>\n",
       "      <th>Ascore</th>\n",
       "      <th>Cscore</th>\n",
       "      <th>...</th>\n",
       "      <th>Ecstasy</th>\n",
       "      <th>Heroin</th>\n",
       "      <th>Ketamine</th>\n",
       "      <th>Legalh</th>\n",
       "      <th>LSD</th>\n",
       "      <th>Meth</th>\n",
       "      <th>Mushrooms</th>\n",
       "      <th>Nicotine</th>\n",
       "      <th>Semer</th>\n",
       "      <th>VSA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.497881</td>\n",
       "      <td>F</td>\n",
       "      <td>-0.059210</td>\n",
       "      <td>UK</td>\n",
       "      <td>WA</td>\n",
       "      <td>0.312874</td>\n",
       "      <td>-0.575450</td>\n",
       "      <td>-0.583314</td>\n",
       "      <td>-0.916984</td>\n",
       "      <td>-0.006649</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.078537</td>\n",
       "      <td>M</td>\n",
       "      <td>1.984368</td>\n",
       "      <td>UK</td>\n",
       "      <td>WH</td>\n",
       "      <td>-0.678251</td>\n",
       "      <td>1.938848</td>\n",
       "      <td>1.435328</td>\n",
       "      <td>0.760954</td>\n",
       "      <td>-0.142770</td>\n",
       "      <td>...</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.497881</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.059210</td>\n",
       "      <td>UK</td>\n",
       "      <td>WH</td>\n",
       "      <td>-0.467254</td>\n",
       "      <td>0.805225</td>\n",
       "      <td>-0.847320</td>\n",
       "      <td>-1.620897</td>\n",
       "      <td>-1.014499</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.951966</td>\n",
       "      <td>F</td>\n",
       "      <td>1.163659</td>\n",
       "      <td>UK</td>\n",
       "      <td>WH</td>\n",
       "      <td>-0.148818</td>\n",
       "      <td>-0.806145</td>\n",
       "      <td>-0.019283</td>\n",
       "      <td>0.590423</td>\n",
       "      <td>0.584891</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.497881</td>\n",
       "      <td>F</td>\n",
       "      <td>1.984368</td>\n",
       "      <td>UK</td>\n",
       "      <td>WH</td>\n",
       "      <td>0.735444</td>\n",
       "      <td>-1.633389</td>\n",
       "      <td>-0.451736</td>\n",
       "      <td>-0.301721</td>\n",
       "      <td>1.306119</td>\n",
       "      <td>...</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Age Gender  Education Country Ethnicity    Nscore    Escore  \\\n",
       "ID                                                                     \n",
       "1   0.497881      F  -0.059210      UK        WA  0.312874 -0.575450   \n",
       "2  -0.078537      M   1.984368      UK        WH -0.678251  1.938848   \n",
       "3   0.497881      M  -0.059210      UK        WH -0.467254  0.805225   \n",
       "4  -0.951966      F   1.163659      UK        WH -0.148818 -0.806145   \n",
       "5   0.497881      F   1.984368      UK        WH  0.735444 -1.633389   \n",
       "\n",
       "      Oscore    Ascore    Cscore  ...  Ecstasy  Heroin Ketamine Legalh  LSD  \\\n",
       "ID                                ...                                         \n",
       "1  -0.583314 -0.916984 -0.006649  ...      CL0     CL0      CL0    CL0  CL0   \n",
       "2   1.435328  0.760954 -0.142770  ...      CL4     CL0      CL2    CL0  CL2   \n",
       "3  -0.847320 -1.620897 -1.014499  ...      CL0     CL0      CL0    CL0  CL0   \n",
       "4  -0.019283  0.590423  0.584891  ...      CL0     CL0      CL2    CL0  CL0   \n",
       "5  -0.451736 -0.301721  1.306119  ...      CL1     CL0      CL0    CL1  CL0   \n",
       "\n",
       "   Meth Mushrooms Nicotine Semer  VSA  \n",
       "ID                                     \n",
       "1   CL0       CL0      CL2   CL0  CL0  \n",
       "2   CL3       CL0      CL4   CL0  CL0  \n",
       "3   CL0       CL1      CL0   CL0  CL0  \n",
       "4   CL0       CL0      CL2   CL0  CL0  \n",
       "5   CL0       CL2      CL2   CL0  CL0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def nominal_quantification(df, nominal_columns, non_nominal_columns):\n",
    "    \"\"\"\n",
    "    Quantify nominal features using PCA and centroids.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        nominal_columns (list): List of nominal columns to be quantified.\n",
    "        non_nominal_columns (list): List of non-nominal columns used for PCA.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with nominal features quantified.\n",
    "        dict: A dictionary of centroids for each nominal feature.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Exclude nominal features and apply PCA to the remaining features\n",
    "    retained_features = df[non_nominal_columns]\n",
    "    \n",
    "    # Perform PCA on the retained features\n",
    "    pca = PCA()\n",
    "    pca_retained = pca.fit_transform(retained_features)\n",
    "\n",
    "    # Select components based on Kaiser’s rule (keep components with eigenvalues > 1)\n",
    "    eigenvalues = pca.explained_variance_\n",
    "    informative_components = np.where(eigenvalues > 1)[0]\n",
    "    \n",
    "    centroids_dict = {}\n",
    "\n",
    "    # Step 2: Calculate centroids for each nominal feature\n",
    "    for col in nominal_columns:\n",
    "        categories = df[col].unique()\n",
    "        centroids = []\n",
    "\n",
    "        for category in categories:\n",
    "            # Get the PCA projections for this category\n",
    "            category_data = pca_retained[df[col] == category]\n",
    "\n",
    "            if category_data.shape[0] == 0:  # Skip if no data for category\n",
    "                continue\n",
    "            \n",
    "            # Calculate the centroid of the category\n",
    "            centroid = category_data.mean(axis=0)  # Shape will be (n_components,)\n",
    "            centroids.append(centroid)\n",
    "\n",
    "        if centroids:\n",
    "            centroids = np.array(centroids)  # Shape will be (n_categories, n_components)\n",
    "            centroids_dict[col] = centroids\n",
    "            \n",
    "            # Step 3: Calculate the first principal component of centroids\n",
    "            if informative_components.size > 0:\n",
    "                # Get the first principal component\n",
    "                first_pc = pca.components_[informative_components[0]].reshape(-1, 1)  # Ensure it's a column vector\n",
    "                \n",
    "                # Step 4: Project centroids onto the first principal component\n",
    "                projections = centroids @ first_pc  # Matrix multiplication\n",
    "                \n",
    "                # Step 5: Assign quantified values back to the DataFrame\n",
    "                unique_categories = categories\n",
    "                quant_map = dict(zip(unique_categories, projections.flatten()))  # Flatten projections to 1D\n",
    "                \n",
    "                # Replace the original nominal feature with the quantified version\n",
    "                df[col] = df[col].map(quant_map)\n",
    "\n",
    "    return df, centroids_dict\n",
    "\n",
    "# Usage example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gender': array([[-0.40540129,  0.07116953,  0.16601383, -0.05103606,  0.04036753,\n",
      "        -0.09551151,  0.10582806,  0.10449228, -0.02378647],\n",
      "       [ 0.40497139, -0.07109406, -0.16583778,  0.05098194, -0.04032473,\n",
      "         0.09541022, -0.10571584, -0.10438148,  0.02376125]]), 'Country': array([[-0.57345682,  0.10001225, -0.05531235, -0.11129554,  0.14308995,\n",
      "        -0.08245525,  0.02714553,  0.08492196, -0.00753269],\n",
      "       [ 0.17388188,  0.25387784, -0.27131481,  0.03248144, -0.06529933,\n",
      "         0.20201995,  0.18107971, -0.07339528,  0.00896023],\n",
      "       [ 0.86652298, -0.16392726,  0.15335041,  0.16524871, -0.21252235,\n",
      "         0.09212734, -0.0053193 , -0.11937831, -0.01230422],\n",
      "       [ 0.59526382, -0.12971857,  0.00539646,  0.01017082, -0.21015231,\n",
      "         0.15858254, -0.31554634, -0.11631088,  0.02502483],\n",
      "       [ 0.45929539, -0.33273178, -0.06920693,  0.18386412,  0.04254336,\n",
      "        -0.03764248, -0.02531976, -0.09690138,  0.08039684],\n",
      "       [ 0.1515467 , -0.18451979, -0.15531632,  0.41593573, -0.0711476 ,\n",
      "         0.03175742, -0.14953222,  0.2649066 ,  0.2914081 ],\n",
      "       [ 0.56677652,  0.35440549,  0.42820455,  0.37511904, -0.2812663 ,\n",
      "        -0.02451534,  0.09227144, -0.42403902,  0.16310404]]), 'Ethnicity': array([[ 3.87420330e-01, -1.62859146e-01,  2.10724883e-01,\n",
      "        -7.10816792e-02, -1.84924736e-01, -2.05919713e-01,\n",
      "         1.80437559e-01,  2.58347619e-01, -1.59650490e-01],\n",
      "       [-3.34760154e-03,  2.04645912e-02, -3.92348516e-03,\n",
      "         9.27901845e-03,  1.22547090e-02,  1.81106269e-02,\n",
      "        -4.73277207e-03, -1.63084359e-03,  6.02469303e-03],\n",
      "       [ 3.88523439e-01, -4.89496582e-01,  5.30878895e-02,\n",
      "        -2.00007734e-03, -2.03612421e-01, -1.43933736e-02,\n",
      "         1.32585583e-01, -1.49324121e-01,  1.30369244e-02],\n",
      "       [ 4.99559149e-02, -2.39523444e-01,  8.00622008e-02,\n",
      "         3.23120842e-02, -1.09742133e-01, -3.48705068e-01,\n",
      "         8.53745566e-02,  5.23993944e-02, -9.60222257e-02],\n",
      "       [-2.49807769e-01, -6.61040627e-02,  1.43993185e-01,\n",
      "        -4.53918838e-01, -6.96508442e-02, -4.36041738e-01,\n",
      "        -1.14956255e-01,  1.64022650e-01, -6.91889657e-02],\n",
      "       [-8.26532084e-01,  1.87485516e-01, -1.54007738e-01,\n",
      "        -7.01277422e-02, -3.42339425e-03, -2.14075801e-01,\n",
      "        -7.03116970e-02,  1.19209921e-02, -1.04916469e-01],\n",
      "       [ 2.10131128e+00, -2.60492279e-01, -3.57817232e-01,\n",
      "        -3.14136559e-01, -1.44428520e-01, -2.49804516e-01,\n",
      "        -7.32058250e-02,  4.46516228e-01, -2.69729112e-01]])}\n"
     ]
    }
   ],
   "source": [
    "nominal_columns = ['Gender', 'Country', 'Ethnicity']\n",
    "non_nominal_columns = ['Age', 'Education', 'Nscore', 'Escore', 'Oscore', 'Ascore', 'Cscore', 'SS', 'Impulsivity']\n",
    "quantified_df, centroids_dict = nominal_quantification(df, nominal_columns, non_nominal_columns)\n",
    "print(centroids_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not touch this code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
